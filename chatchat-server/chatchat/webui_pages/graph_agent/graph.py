from langchain_core.tools import BaseTool
from langchain_openai import ChatOpenAI
from streamlit_extras.bottom_container import bottom

from chatchat.server.agent.graphs_factory.graphs_registry import (
    list_graph_titles_by_label,
    get_graph_class_by_label_and_title
)
from chatchat.webui_pages.utils import *

from chatchat.server.utils import (
    build_logger,
    get_tool,
    list_tools,
    create_agent_models,
    get_checkpointer
)

logger = build_logger()


# @st.dialog("è¾“å…¥åˆå§‹åŒ–å†…å®¹", width="large")
# def article_generation_init_setting():
#     article_links = st.text_area("æ–‡ç« é“¾æ¥")
#     image_links = st.text_area("å›¾ç‰‡é“¾æ¥")
#
#     if st.button("ç¡®è®¤"):
#         st.session_state["article_links"] = article_links
#         st.session_state["image_links"] = image_links
#         # å°† article_generation_init_break_point çŠ¶æ€æ‰­è½¬ä¸º True, åç»­å°†è¿›è¡Œ update_state åŠ¨ä½œ
#         st.session_state["article_generation_init_break_point"] = True
#
#         user_input = (f"æ–‡ç« é“¾æ¥: {article_links}\n"
#                       f"å›¾ç‰‡é“¾æ¥: {image_links}")
#         with st.chat_message("user"):
#             st.markdown(user_input)
#         st.session_state.messages.append({
#             "role": "user",
#             "content": user_input,
#             "type": "text"  # æ ‡è¯†ä¸ºæ–‡æœ¬ç±»å‹
#         })
#
#         st.rerun()


# @st.dialog("å¼€å§‹æ”¹å†™æ–‡ç« ", width="large")
# def article_generation_start_setting():
#     cols = st.columns(3)
#     platforms = ["æ‰€æœ‰"] + list(get_config_platforms())
#     platform = cols[0].selectbox("æ¨¡å‹å¹³å°è®¾ç½®(Platform)", platforms)
#     llm_models = list(
#         get_config_models(
#             model_type="llm", platform_name=None if platform == "æ‰€æœ‰" else platform
#         )
#     )
#     llm_model = cols[1].selectbox("æ¨¡å‹è®¾ç½®(LLM)", llm_models)
#     temperature = cols[2].slider("æ¸©åº¦è®¾ç½®(Temperature)", 0.0, 1.0, value=st.session_state["temperature"])
#     with st.container(height=300):
#         st.markdown(st.session_state["article_list"])
#     prompt = st.text_area("æŒ‡ä»¤(Prompt):", value="1.å°†ä¸Šè¿°æä¾›çš„æ–‡ç« å†…å®¹åˆ—è¡¨,å„è‡ªæç‚¼å‡ºæçº²;\n"
#                                                  "2.å°†æçº²åˆ—è¡¨æ•´åˆæˆä¸€ç¯‡æ–‡ç« çš„æçº²;\n"
#                                                  "3.æŒ‰ç…§æ•´åˆåçš„æçº², ç”Ÿæˆä¸€ç¯‡æ–°çš„æ–‡ç« , å­—æ•°è¦æ±‚ 500å­—å·¦å³;\n"
#                                                  "4.åªéœ€è¦è¿”å›æœ€åçš„æ–‡ç« å†…å®¹å³å¯.")
#
#     if st.button("å¼€å§‹ç¼–å†™"):
#         st.session_state["platform"] = platform
#         st.session_state["llm_model"] = llm_model
#         st.session_state["temperature"] = temperature
#         st.session_state["prompt"] = prompt
#         # å°† article_generation_start_break_point çŠ¶æ€æ‰­è½¬ä¸º True, åç»­å°†è¿›è¡Œ update_state åŠ¨ä½œ
#         st.session_state["article_generation_start_break_point"] = True
#
#         user_input = (f"æ¨¡å‹: {llm_model}\n"
#                       f"æ¸©åº¦: {temperature}\n"
#                       f"æŒ‡ä»¤: {prompt}")
#         with st.chat_message("user"):
#             st.markdown(user_input)
#         st.session_state.messages.append({
#             "role": "user",
#             "content": user_input,
#             "type": "text"  # æ ‡è¯†ä¸ºæ–‡æœ¬ç±»å‹
#         })
#
#         st.rerun()


# @st.dialog("æ–‡ç« é‡å†™ç¡®è®¤", width="large")
# def article_generation_repeat_setting():
#     cols = st.columns(3)
#     platforms = ["æ‰€æœ‰"] + list(get_config_platforms())
#     platform = cols[0].selectbox("æ¨¡å‹å¹³å°è®¾ç½®(Platform)", platforms)
#     llm_models = list(
#         get_config_models(
#             model_type="llm", platform_name=None if platform == "æ‰€æœ‰" else platform
#         )
#     )
#     llm_model = cols[1].selectbox("æ¨¡å‹è®¾ç½®(LLM)", llm_models)
#     temperature = cols[2].slider("æ¸©åº¦è®¾ç½®(Temperature)", 0.0, 1.0, value=st.session_state["temperature"])
#     with st.container(height=300):
#         st.markdown(st.session_state["article"])
#     prompt = st.text_area("æŒ‡ä»¤(Prompt):", value="è¯·ç»§ç»­ä¼˜åŒ–, æœ€ååªéœ€è¦è¿”å›æ–‡ç« å†…å®¹.")
#
#     if st.button("ç¡®è®¤-éœ€è¦é‡å†™"):
#         st.session_state["platform"] = platform
#         st.session_state["llm_model"] = llm_model
#         st.session_state["temperature"] = temperature
#         st.session_state["prompt"] = prompt
#         st.session_state["article_generation_repeat_break_point"] = True
#
#         user_input = (f"æ¨¡å‹: {llm_model}\n"
#                       f"æ¸©åº¦: {temperature}\n"
#                       f"æŒ‡ä»¤: {prompt}")
#         with st.chat_message("user"):
#             st.markdown(user_input)
#         st.session_state.messages.append({
#             "role": "user",
#             "content": user_input,
#             "type": "text"  # æ ‡è¯†ä¸ºæ–‡æœ¬ç±»å‹
#         })
#         st.rerun()
#     if st.button("ç¡®è®¤-ä¸éœ€è¦é‡å†™"):
#         # å¦‚æœä¸éœ€è¦ç»§ç»­æ”¹å†™, åˆ™å›ºå®š prompt å¦‚ä¸‹
#         prompt = "ä¸éœ€è¦ç»§ç»­æ”¹å†™æ–‡ç« ."
#
#         st.session_state["platform"] = platform
#         st.session_state["llm_model"] = llm_model
#         st.session_state["temperature"] = temperature
#         st.session_state["prompt"] = prompt
#         st.session_state["article_generation_repeat_break_point"] = True
#         # langgraph é€€å‡ºå¾ªç¯çš„åˆ¤æ–­æ¡ä»¶
#         st.session_state["is_article_generation_complete"] = True
#
#         user_input = (f"æ¨¡å‹: {llm_model}\n"
#                       f"æ¸©åº¦: {temperature}\n"
#                       f"æŒ‡ä»¤: {prompt}")
#         with st.chat_message("user"):
#             st.markdown(user_input)
#         st.session_state.messages.append({
#             "role": "user",
#             "content": user_input,
#             "type": "text"  # æ ‡è¯†ä¸ºæ–‡æœ¬ç±»å‹
#         })
#         st.rerun()


async def create_graph(
        graph_class: Type[Graph],
        graph_input: Any,
        graph_config: dict,
        graph_llm: ChatOpenAI,
        graph_tools: list[BaseTool],
        graph_history_len: int,
):
    if st.session_state["checkpoint_type"] == "memory":
        if "memory" not in st.session_state:
            st.session_state["memory"] = get_checkpointer()
        checkpoint = st.session_state["memory"]
        graph_class = graph_class(llm=graph_llm,
                                  tools=graph_tools,
                                  history_len=graph_history_len,
                                  checkpoint=checkpoint)
        graph = graph_class.get_graph()
        if not graph:
            raise ValueError(f"Graph '{graph_class}' is not registered.")
        await process_graph(graph_class=graph_class, graph=graph, graph_input=graph_input, graph_config=graph_config)
    elif st.session_state["checkpoint_type"] == "sqlite":
        checkpoint_class = get_checkpointer()
        async with checkpoint_class as checkpoint:
            graph_class = graph_class(llm=graph_llm,
                                      tools=graph_tools,
                                      history_len=graph_history_len,
                                      checkpoint=checkpoint)
            graph = graph_class.get_graph()
            if not graph:
                raise ValueError(f"Graph '{graph_class}' is not registered.")
            await process_graph(graph_class=graph_class, graph=graph, graph_input=graph_input, graph_config=graph_config)
    elif st.session_state["checkpoint_type"] == "postgres":
        from psycopg_pool import AsyncConnectionPool
        from langgraph.checkpoint.postgres.aio import AsyncPostgresSaver
        async with AsyncConnectionPool(
            conninfo=Settings.basic_settings.POSTGRESQL_GRAPH_DATABASE_URI,
            max_size=Settings.basic_settings.POSTGRESQL_GRAPH_CONNECTION_POOLS_MAX_SIZE,
            kwargs=Settings.basic_settings.POSTGRESQL_GRAPH_CONNECTION_POOLS_KWARGS,
        ) as pool:
            checkpoint = AsyncPostgresSaver(pool)
            # NOTE: you need to call .setup() the first time you're using your checkpointer
            await checkpoint.setup()
            graph_class = graph_class(llm=graph_llm,
                                      tools=graph_tools,
                                      history_len=graph_history_len,
                                      checkpoint=checkpoint)
            graph = graph_class.get_graph()
            if not graph:
                raise ValueError(f"Graph '{graph_class}' is not registered.")
            await process_graph(graph_class=graph_class, graph=graph, graph_input=graph_input, graph_config=graph_config)


async def update_state(graph: CompiledStateGraph, graph_config: Dict, update_message: Dict, as_node: str):
    # rich.print(update_message)  # debug

    # print("--State before update--")
    # # ä½¿ç”¨å¼‚æ­¥å‡½æ•°æ¥è·å–çŠ¶æ€å†å²
    # state_history = []
    # async for state in graph.aget_state_history(graph_config):
    #     state_history.append(state)
    # rich.print(state_history)

    # æ›´æ–°çŠ¶æ€
    await graph.aupdate_state(config=graph_config,
                              values=update_message,
                              as_node=as_node)

    # print("--State after update--")
    # # å†æ¬¡æ‰“å°çŠ¶æ€å†å²
    # state_history = []
    # async for state in graph.aget_state_history(graph_config):
    #     state_history.append(state)
    # rich.print(state_history)


async def graph_agent_page():
    # åˆå§‹åŒ–
    init_conversation_id()
    if "article_generation_init_break_point" not in st.session_state:
        st.session_state["article_generation_init_break_point"] = False
    if "article_generation_start_break_point" not in st.session_state:
        st.session_state["article_generation_start_break_point"] = False
    if "article_generation_repeat_break_point" not in st.session_state:
        st.session_state["article_generation_repeat_break_point"] = False
    if "is_article_generation_complete" not in st.session_state:
        st.session_state["is_article_generation_complete"] = False

    with st.sidebar:
        tabs_1 = st.tabs(["å·¥å…·è®¾ç½®"])
        with tabs_1[0]:
            agent_graph_names = list_graph_titles_by_label(label="agent")
            selected_graph = st.selectbox(
                "é€‰æ‹©å·¥ä½œæµ",
                agent_graph_names,
                format_func=lambda x: x,
                key="selected_graph",
                help="å¿…é€‰ï¼Œä¸åŒçš„å·¥ä½œæµçš„åç«¯ agent çš„é€»è¾‘ä¸åŒï¼Œä»…æ”¯æŒå•é€‰"
            )

            tools_list = list_tools()
            if selected_graph == "æ•°æ®åº“æŸ¥è¯¢æœºå™¨äºº[Beta]":
                selected_tools = st.multiselect(
                    label="é€‰æ‹©å·¥å…·",
                    options=["query_sql_data"],
                    format_func=lambda x: tools_list[x]["title"],
                    key="selected_tools",
                    default="query_sql_data",
                    help="ä»…å¯é€‰æ‹© SQLæŸ¥è¯¢å·¥å…·"
                )
            elif selected_graph == "è‡ªæˆ‘åæ€æœºå™¨äºº[Beta]":
                selected_tools = st.multiselect(
                    label="é€‰æ‹©å·¥å…·",
                    options=list(tools_list),
                    format_func=lambda x: tools_list[x]["title"],
                    key="selected_tools",
                    default="search_internet",
                    help="æ”¯æŒå¤šé€‰"
                )
            else:
                # selected_tools demo: ['search_internet', 'search_youtube']
                selected_tools = st.multiselect(
                    label="é€‰æ‹©å·¥å…·",
                    options=list(tools_list),
                    format_func=lambda x: tools_list[x]["title"],
                    key="selected_tools",
                    help="æ”¯æŒå¤šé€‰"
                )

            selected_tool_configs = {
                name: tool["config"]
                for name, tool in tools_list.items()
                if name in selected_tools
            }

        tabs_2 = st.tabs(["èŠå¤©è®¾ç½®"])
        with tabs_2[0]:
            history_len = st.number_input("å†å²å¯¹è¯è½®æ•°", 0, 20, key="history_len")

        st.tabs(["å·¥ä½œæµæµç¨‹å›¾"])

    selected_tools_configs = list(selected_tool_configs)

    if selected_graph == "article_generation":
        st.title("è‡ªåª’ä½“æ–‡ç« ç”Ÿæˆ")
        with st.chat_message(name="assistant", avatar=st.session_state["assistant_avatar"]):
            st.write("Hello ğŸ‘‹ğŸ˜Šï¼Œæˆ‘æ˜¯è‡ªåª’ä½“æ–‡ç« ç”Ÿæˆ Agentï¼Œè¾“å…¥ä»»æ„å†…å®¹ä»¥å¯åŠ¨å·¥ä½œæµï½")
    elif selected_graph == "æ•°æ®åº“æŸ¥è¯¢æœºå™¨äºº[Beta]":
        st.title("æ•°æ®åº“æŸ¥è¯¢")
        with st.chat_message(name="assistant", avatar=st.session_state["assistant_avatar"]):
            st.write("Hello ğŸ‘‹ğŸ˜Šï¼Œæˆ‘æ˜¯æ•°æ®åº“æŸ¥è¯¢æœºå™¨äººï¼Œè¾“å…¥ä½ æƒ³æŸ¥è¯¢çš„å†…å®¹ï½")
    else:
        st.title("LLM èŠå¤©")
        with st.chat_message(name="assistant", avatar=st.session_state["assistant_avatar"]):
            st.write("Hello ğŸ‘‹ğŸ˜Šï¼Œæˆ‘æ˜¯èŠå¤©æœºå™¨äººï¼Œè¯•ç€è¾“å…¥ä»»ä½•å†…å®¹å’Œæˆ‘èŠå¤©å‘¦ï½ï¼ˆps: å¯å°è¯•é€‰æ‹©å¤šç§å·¥å…·ï¼‰")

    with bottom():
        cols = st.columns([1, 0.2, 15, 1])
        if cols[0].button(":gear:", help="æ¨¡å‹é…ç½®"):
            llm_model_setting()
        if cols[-1].button(":wastebasket:", help="æ¸…ç©ºå¯¹è¯"):
            st.session_state["messages"] = []
            st.rerun()
        if selected_graph == "article_generation":
            user_input = cols[2].chat_input("å°è¯•è¾“å…¥ä»»ä½•å†…å®¹å’Œæˆ‘èŠå¤©å‘¦ (æ¢è¡Œ:Shift+Enter)")
        elif selected_graph == "æ•°æ®åº“æŸ¥è¯¢æœºå™¨äºº[Beta]":
            user_input = cols[2].chat_input("å°è¯•è¾“å…¥ä»»ä½•å†…å®¹å’Œæˆ‘èŠå¤©å‘¦ (æ¢è¡Œ:Shift+Enter)")
        else:
            user_input = cols[2].chat_input("å°è¯•è¾“å…¥ä»»ä½•å†…å®¹å’Œæˆ‘èŠå¤©å‘¦ (æ¢è¡Œ:Shift+Enter)")

    # get_tool() æ˜¯æ‰€æœ‰å·¥å…·çš„åç§°å’Œå¯¹è±¡çš„ dict çš„åˆ—è¡¨
    all_tools = get_tool().values()
    tools = [tool for tool in all_tools if tool.name in selected_tools_configs]

    # åˆ›å»º llm å®ä¾‹
    # todo: max_tokens è¿™é‡Œæœ‰é—®é¢˜, None åº”è¯¥æ˜¯ä¸é™åˆ¶, ä½†æ˜¯ç›®å‰ llm ç»“æœä¸º 4096
    llm_model = st.session_state["llm_model"]
    llm = create_agent_models(configs=None,
                              model=llm_model,
                              max_tokens=None,
                              temperature=st.session_state["temperature"],
                              stream=st.session_state["streaming"])
    st.toast(f"å·²åŠ è½½ LLM: {llm_model}")
    logger.info(f"Loaded llm: {llm}")

    # åˆ›å»º langgraph å®ä¾‹
    graph_class = get_graph_class_by_label_and_title(label="agent", title=selected_graph)

    graph_instance = st.session_state["graph_dict"].get(selected_graph)
    if graph_instance is None:
        graph_png_image = get_img_base64(f"{selected_graph}.jpg")
        # if not graph_png_image:
        #     graph_png_image = graph.get_graph().draw_mermaid_png()
        #     logger.warning(f"The graph({selected_graph}) flowchart is not found in img, use graph.draw_mermaid_png() to get it.")
        st.session_state["graph_dict"][selected_graph] = {
            "graph_class": graph_class,
            "graph_image": graph_png_image,
        }
    st.toast(f"å·²åŠ è½½å·¥ä½œæµ: {selected_graph}")

    # langgraph é…ç½®æ–‡ä»¶
    graph_config = {
        "configurable": {
            "thread_id": st.session_state["conversation_id"]
        },
    }
    logger.info(f"Loaded graph: '{selected_graph}', configurable: '{graph_config}'")

    st.sidebar.image(st.session_state["graph_dict"][selected_graph]["graph_image"], use_column_width=True)

    if selected_graph == "article_generation":
        # åˆå§‹åŒ–æ–‡ç« å’Œå›¾ç‰‡ä¿¡æ¯
        if "article_links" not in st.session_state:
            st.session_state["article_links"] = ""
        if "image_links" not in st.session_state:
            st.session_state["image_links"] = ""
        if "article_links_list" not in st.session_state:
            st.session_state["article_links_list"] = []
        if "image_links_list" not in st.session_state:
            st.session_state["image_links_list"] = []

    # å‰ç«¯å­˜å‚¨å†å²æ¶ˆæ¯(ä»…ä½œä¸º st.rerun() æ—¶çš„ UI å±•ç¤º)
    # ä¸´æ—¶åˆ—è¡¨ï¼Œç”¨äºæ”¶é›† assistant çš„æ¶ˆæ¯
    assistant_messages = []

    # éå† st.session_state.messages å¹¶å±•ç¤ºæ¶ˆæ¯
    for message in st.session_state.messages:
        role = message['role']
        content = message['content']
        is_last_message = message.get('is_last_message', False)

        if role == 'user':
            # å±•ç¤º user æ¶ˆæ¯
            with st.chat_message("user"):
                st.markdown(content)
        elif role == 'assistant':
            # æ”¶é›† assistant æ¶ˆæ¯
            assistant_messages.append(message)
            # å¦‚æœæ˜¯æœ€åä¸€æ¡ assistant æ¶ˆæ¯ï¼Œç«‹å³å±•ç¤º
            if is_last_message:
                with st.chat_message(name="assistant", avatar=st.session_state["assistant_avatar"]):
                    for msg in assistant_messages:
                        if msg['is_last_message']:
                            st.markdown(msg['content'])
                        else:
                            with st.status(msg['node'], expanded=True) as status:
                                st.json(msg['content'], expanded=True)
                                status.update(
                                    label=msg['node'], state="complete", expanded=False
                                )
                # æ¸…ç©ºä¸´æ—¶åˆ—è¡¨
                assistant_messages = []

    # å¯¹è¯ä¸»æµç¨‹
    if user_input:
        st.session_state.messages.append(create_chat_message(
            role="user",
            content=user_input,
            node=None,
            expanded=None,
            type="text",
            is_last_message=True
        ))
        with st.chat_message("user"):
            st.markdown(user_input)

        # Run the async function in a synchronous context
        graph_input = {"messages": [("user", user_input)]}
        await create_graph(graph_class=st.session_state["graph_dict"][selected_graph]["graph_class"],
                           graph_input=graph_input,
                           graph_config=graph_config,
                           graph_llm=llm,
                           graph_tools=tools,
                           graph_history_len=history_len)
        st.rerun()  # Clear stale containers
